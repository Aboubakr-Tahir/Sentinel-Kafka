# ğŸ›¡ï¸ Sentinel - Real-time System Monitoring

Sentinel est une solution de monitoring systÃ¨me temps rÃ©el basÃ©e sur **Apache Kafka**. Elle collecte des mÃ©triques systÃ¨me (CPU, RAM, Disque) via Python, les traite via un Consumer Scala (Kafka Streams), et archive les alertes critiques via Kafka Connect.

## ğŸ¥ DÃ©mo du Dashboard

[Voir la vidÃ©o de dÃ©monstration](assets/kakfa_showcase.mp4)

## ğŸ—ï¸ Architecture

Cette architecture met en Å“uvre les concepts de **Producer**, **Consumer Groups**, **Kafka Streams** et **Kafka Connect** pour crÃ©er un pipeline de donnÃ©es robuste et scalable.

![Architecture de Sentinel](assets/architecture_design.png)

### Flux de donnÃ©es :
1.  **System Metrics Collector (Producer Python)** : Utilise `psutil` pour collecter les mÃ©triques (CPU, RAM, Disk) et les publie sur le topic `system_metrics` (partitionnÃ© par type de ressource).
2.  **Infrastructure Kafka** : Cluster Kafka gÃ©rant la distribution et la rÃ©tention des messages (24h).
3.  **Alert Engine (Kafka Streams / Scala)** : Consomme les mÃ©triques en temps rÃ©el (`group-id: alert-group`), applique des fenÃªtres de temps pour analyser les tendances, et produit des alertes sur le topic `system_alerts` en cas de dÃ©passement de seuil.
4.  **Live Dashboard (Consumer Python / Streamlit)** : Consomme les donnÃ©es en temps rÃ©el (`group-id: ui-group`) pour les visualiser sur une interface web interactive.
5.  **Archiver (Kafka Connect)** : Sink Connector qui Ã©coute le topic `system_alerts` et sauvegarde automatiquement les anomalies dans des fichiers logs pour audit ultÃ©rieur.

## PrÃ©requis

*   **Kafka** (v3.x ou 4.x) & Zookeeper (ou KRaft)
*   **Python** 3.8+
*   **SBT** (Scala Build Tool) & **Java** 17+

## Installation & DÃ©marrage

### 1. DÃ©marrer l'infrastructure Kafka
Assurez-vous que Zookeeper et Kafka Broker sont lancÃ©s. CrÃ©ez les topics nÃ©cessaires (si non crÃ©Ã©s automatiquement) :
```bash
kafka-topics.sh --create --topic system_metrics --bootstrap-server localhost:9092
kafka-topics.sh --create --topic system_alerts --bootstrap-server localhost:9092
```

### 2. Configuration Python (Producer & Dashboard)
CrÃ©ez un environnement virtuel et installez les dÃ©pendances :
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Lancer le producteur de mÃ©triques :
```bash
python3 src/producer.py
```

Lancer le Dashboard :
```bash
streamlit run dashboard/app.py
```

### 3. Configuration Scala (Stream Processor)
Compilez et lancez le processeur d'alertes :
```bash
sbt "runMain sentinel.AlertSystem"
```

### 4. Configuration Kafka Connect (Archivage)
Pour sauvegarder les alertes dans un fichier (`tmp/alerts.log`).

1.  Assurez-vous que les plugins nÃ©cessaires sont dans le dossier `plugins/`.
2.  Lancer le connecteur en mode standalone :
```bash
<path-to-kafka>/bin/connect-standalone.sh config/connect-standalone.properties config/alerts-sink.properties
```

## Structure du Projet

```
Sentinel/
â”œâ”€â”€ assets/                  # Images d'architecture et dÃ©mos
â”œâ”€â”€ config/                  # Configurations Kafka Connect (.properties)
â”œâ”€â”€ dashboard/               # Application Streamlit
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ plugins/                 # Dossier pour les JARs Kafka Connect (exclus du git)
â”œâ”€â”€ src/                     # Code source
â”‚   â”œâ”€â”€ producer.py          # Script Python de collecte
â”‚   â”œâ”€â”€ admin_create.py      # Script admin Kafka
â”‚   â””â”€â”€ main/scala/          # AlertSystem.scala (Kafka Consumer)
â”œâ”€â”€ tmp/                     # Dossier de destination des logs (exclus du git)
â”œâ”€â”€ build.sbt                # DÃ©finition du projet Scala
â””â”€â”€ README.md
```
